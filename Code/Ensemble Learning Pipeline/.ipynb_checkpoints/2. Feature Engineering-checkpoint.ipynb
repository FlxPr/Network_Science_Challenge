{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import swifter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper funtion to assign communities\n",
    "def assign_communities(community, df, name):\n",
    "    # Create a new df\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create a new column for communities\n",
    "    df[name] = 0\n",
    "    \n",
    "    for i, com in enumerate(community):\n",
    "        \n",
    "        # Get node indices\n",
    "        com = G.vs[com]['id']\n",
    "        \n",
    "        # Convert strings to integers\n",
    "        com = [int(x) for x in com]\n",
    "        \n",
    "        mask = df['ID'].isin(com)\n",
    "        df.loc[mask, name] = i\n",
    "\n",
    "    return df\n",
    "\n",
    "# A helper funtion to calculate the shortest distance between 2 nodes\n",
    "def helper_shortest_path(x):\n",
    "    try:\n",
    "        source = old2new[x[0]]\n",
    "        destination = old2new[x[1]]\n",
    "        d = G.shortest_paths(source, destination)[0][0]\n",
    "\n",
    "    # When one of the vertices doesnt even exist\n",
    "    except:\n",
    "        d = 999\n",
    "    \n",
    "    # Remove the existing edges between A and B to avoid data leakage (Too computationally expensive)\n",
    "    # if d == 1:\n",
    "    #    G_copy = G.copy()\n",
    "    #    G_copy.delete_edges([(source, destination)])\n",
    "    #    d = G_copy.shortest_paths(source, destination)[0][0]\n",
    "    \n",
    "    # Convert infinity to 999\n",
    "    if d > 999:\n",
    "        d = 999\n",
    "    \n",
    "    return d\n",
    "\n",
    "# A helper funtion to clean author name\n",
    "def clean_author(li):\n",
    "    res = []\n",
    "    \n",
    "    for text in li:\n",
    "        \n",
    "        # Convert to lower cases\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove the part after '('\n",
    "        text = text.split('(')[0]\n",
    "        \n",
    "        # Remove all the punctuations but dots\n",
    "        text = re.sub(r'[^\\w\\s.]','',text)        \n",
    "        \n",
    "        # Remove spaces at the beginning and the end\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Get last names and initials\n",
    "        if text != '':\n",
    "            if '.' in text:\n",
    "                initial = text.split('.')[0][0]\n",
    "            else:\n",
    "                initial = text[0]\n",
    "            # Assume last names always appear after the last dot\n",
    "            last_name = text.split('.')[-1].split(' ')[-1]\n",
    "            text = initial + '. ' + last_name\n",
    "        else:\n",
    "            text = ''\n",
    "        \n",
    "        # Keep only names that are not too short\n",
    "        if len(text) > 4:\n",
    "            res.append(text)\n",
    "        \n",
    "    return res\n",
    "\n",
    "# A helper funtion to calculate the number of intersections\n",
    "def intersection(x):\n",
    "    return len(set(x.iloc[0]).intersection(x.iloc[1]))\n",
    "\n",
    "# A helper funtion to assign similarities between nodes\n",
    "def similarity(x, M):\n",
    "    try:\n",
    "        sim = M[old2new[x[0]]][old2new[x[1]]]\n",
    "    except:\n",
    "        sim = 0\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the files\n",
    "df_train = pd.read_csv('training_set.txt', header=None, sep=' ', names=['X1', 'X2', 'Y'])\n",
    "df_test = pd.read_csv('testing_set.txt', header=None, sep=' ', names=['X1', 'X2'])\n",
    "df_info = pd.read_csv('node_information.csv', header=None, names=['ID', 'Year', 'Title', 'Author', 'Journal', 'Abstract'])\n",
    "\n",
    "df_doc2vec = pd.read_csv('doc2vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a graph for feature extraction - DONE\n",
    "# edges = df_train[df_train['Y']==1]\n",
    "# G = nx.from_pandas_edgelist(edges, 'X1', 'X2', True, create_using=nx.DiGraph())\n",
    "# nx.write_graphml(G,'graph.graphml')\n",
    "\n",
    "# Read the saved graph\n",
    "G = ig.read('graph.graphml',format=\"graphml\")\n",
    "\n",
    "# Construct index mappings\n",
    "n_nodes = len(G.vs)\n",
    "old2new = {int(G.vs['id'][i]):G.vs.indices[i] for i in range(n_nodes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Node Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: RuntimeWarning: closeness centrality is not well-defined for disconnected graphs at /project/vendor/source/igraph/src/centrality.c:2856\n",
      "  \n",
      "/home/jupyter/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: RuntimeWarning: closeness centrality is not well-defined for disconnected graphs at /project/vendor/source/igraph/src/centrality.c:2856\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Get graph attributes - DONE\n",
    "\n",
    "# att = {'in_degree_centrality':G.degree(mode='in'),\n",
    "#         'out_degree_centrality':G.degree(mode='out'),\n",
    "#         'eigenvector_centrality':G.eigenvector_centrality(),\n",
    "#         'in_closeness_centrality': G.closeness(mode='in'),\n",
    "#         'out_closeness_centrality': G.closeness(mode='out'),        \n",
    "#         'betweenness_centrality': G.betweenness(),\n",
    "#         'pagerank': G.pagerank()}\n",
    "\n",
    "# att = pd.DataFrame(att)\n",
    "# att['index'] = G.vs['id']\n",
    "# att['index'] = pd.to_numeric(att['index'])\n",
    "# df_info = df_info.merge(att, left_on='ID', right_on='index', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for titles and abstracts - DONE\n",
    "\n",
    "# df_info = df_info.merge(df_doc2vec, left_on='ID', right_on='ID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Community Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infomap algorithm - DONE\n",
    "\n",
    "# c_infomap = G.community_infomap()\n",
    "# df_info = assign_communities(c_infomap, df_info, 'infomap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Save to the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_info.to_csv('df_info_temp.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Edge Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Shortest Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the shortest path between each node - Done\n",
    "# df_train['shortest_path'] = df_train.swifter.apply(helper_shortest_path, axis=1)\n",
    "# df_test['shortest_path'] = df_test.swifter.apply(helper_shortest_path, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Doc Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity between A and B - Done\n",
    "# def sim_A_B(df, df_doc2vec):    \n",
    "#     Vec = df_doc2vec.set_index('ID')\n",
    "#     Vec_X1 = Vec.loc[df['X1']].values\n",
    "#     Vec_X2 = Vec.loc[df['X2']].values\n",
    "    \n",
    "#     inner_product = np.sum(Vec_X1 * Vec_X2, axis=1)\n",
    "#     leng_X1 = np.sum(Vec_X1 ** 2, axis=1) ** 0.5\n",
    "#     leng_X2 = np.sum(Vec_X2 ** 2, axis=1) ** 0.5\n",
    "#     sim = inner_product/(leng_X1 * leng_X2)\n",
    "    \n",
    "#     res = df.copy()\n",
    "#     res['similarity_A_B'] = sim\n",
    "    \n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = sim_A_B(df_train, df_doc2vec)\n",
    "# df_test = sim_A_B(df_test, df_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity between B and those cited by A\n",
    "# def sim_Aout_B(df, df_doc2vec):\n",
    "#     Vec = df_doc2vec.set_index('ID')\n",
    "    \n",
    "#     # Calculate the mean vectors of those cited by A\n",
    "#     edges = df_train[df_train['Y'] == 1]\n",
    "#     Vec_Aout = edges.groupby('X1')['X2'].unique().reset_index().set_index('X1')\n",
    "#     temp = Vec_Aout['X2'].map(lambda x:Vec.loc[x].mean().values).to_list()\n",
    "#     Vec_X1 = pd.DataFrame(temp, index=Vec_Aout.index, columns=Vec.columns)\n",
    "    \n",
    "#     # Calculate the similarity between B and those cited by A\n",
    "#     Vec_X1 = Vec_X1.loc[df['X1']].values\n",
    "#     Vec_X2 = Vec.loc[df['X2']].values\n",
    "    \n",
    "#     inner_product = np.sum(Vec_X1 * Vec_X2, axis=1)\n",
    "#     leng_X1 = np.sum(Vec_X1 ** 2, axis=1) ** 0.5\n",
    "#     leng_X2 = np.sum(Vec_X2 ** 2, axis=1) ** 0.5\n",
    "#     sim = inner_product/(leng_X1 * leng_X2)\n",
    "    \n",
    "#     res = df.copy()\n",
    "#     res['similarity_Aout_B'] = sim\n",
    "    \n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/ipykernel_launcher.py:12: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# df_train = sim_Aout_B(df_train, df_doc2vec)\n",
    "# df_test = sim_Aout_B(df_test, df_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity between A and those citing B\n",
    "# def sim_A_Bin(df, df_doc2vec):\n",
    "#     Vec = df_doc2vec.set_index('ID')\n",
    "    \n",
    "#     # Calculate the mean vectors of those cited by A\n",
    "#     edges = df_train[df_train['Y'] == 1]\n",
    "#     Vec_Bin = edges.groupby('X2')['X1'].unique().reset_index().set_index('X2')\n",
    "#     temp = Vec_Bin['X1'].map(lambda x:Vec.loc[x].mean().values).to_list()\n",
    "#     Vec_X2 = pd.DataFrame(temp, index=Vec_Bin.index, columns=Vec.columns)\n",
    "    \n",
    "#     # Calculate the similarity between B and those cited by A\n",
    "#     Vec_X1 = Vec.loc[df['X1']].values\n",
    "#     Vec_X2 = Vec_X2.loc[df['X2']].values\n",
    "    \n",
    "#     inner_product = np.sum(Vec_X1 * Vec_X2, axis=1)\n",
    "#     leng_X1 = np.sum(Vec_X1 ** 2, axis=1) ** 0.5\n",
    "#     leng_X2 = np.sum(Vec_X2 ** 2, axis=1) ** 0.5\n",
    "#     sim = inner_product/(leng_X1 * leng_X2)\n",
    "    \n",
    "#     res = df.copy()\n",
    "#     res['similarity_A_Bin'] = sim\n",
    "    \n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/ipykernel_launcher.py:13: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# df_train = sim_A_Bin(df_train, df_doc2vec)\n",
    "# df_test = sim_A_Bin(df_test, df_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity between those cited by A and those citing B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Author reference frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many common authors between A and B, as well as how many times A has cited B's authors before\n",
    "# def author_frequency(df):\n",
    "#     df = df.copy()\n",
    "    \n",
    "#     # Create an edge list\n",
    "#     edges = df_train[df_train['Y'] == 1].reset_index(drop=True)\n",
    "\n",
    "#     # Clean author names\n",
    "#     df_info2 = df_info.copy() \n",
    "#     aut = df_info2['Author'].fillna('').str.split(',')\n",
    "#     df_info2['Author'] = aut.map(clean_author)\n",
    "#     df_info2 = df_info2.set_index('ID')\n",
    "\n",
    "#     # Find out author lists for each edge\n",
    "#     edges['aut1'] = df_info2.loc[edges['X1']]['Author'].reset_index(drop=True)\n",
    "#     edges['aut2'] = df_info2.loc[edges['X2']]['Author'].reset_index(drop=True)\n",
    "#     temp = edges.groupby('X1')['aut2'].apply(lambda x:[j for i in x.to_list() for j in i])\n",
    "\n",
    "#     df['aut1'] = df_info2.loc[df['X1']]['Author'].reset_index(drop=True)\n",
    "#     df['aut2'] = df_info2.loc[df['X2']]['Author'].reset_index(drop=True)\n",
    "#     df['a_out_aut'] = temp.loc[df['X1']].reset_index(drop=True)\n",
    "\n",
    "#     # Find out the number of common authors\n",
    "#     df['aut_common'] = df[['aut1', 'aut2']].swifter.apply(intersection, axis=1)\n",
    "\n",
    "#     # Find out the number of times A has cited the work of the authors of B\n",
    "#     df['a_out_aut'][df['a_out_aut'].isna()] = ''\n",
    "#     df['n_previously_cited'] = df[['a_out_aut', 'aut2']].swifter.apply(lambda x:sum(el in x[1] for el in x[0]), axis=1)\n",
    "    \n",
    "#     # Drop useless columns\n",
    "#     df = df.drop(['aut1','aut2','a_out_aut'], axis=1)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/ipykernel_launcher.py:20: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b6b18cddf6431280688ecb93e07f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=32648.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7ccc63027e431485af5540cef2b1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=32648.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28142716c14d42da8e6affe815da6dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=615512.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20f2086ee11447a98a6d20420d4cc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=615512.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# df_test = author_frequency(df_test)\n",
    "# df_train = author_frequency(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Reverse Citation (if B has cited A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1b251c557246818249d84ee75f9d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dask Apply', max=32.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0a83697efa4ec9b26e131ae4b097b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dask Apply', max=32.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Return 1 if B has cited A - Done\n",
    "# edges = df_train[df_train['Y'] == 1]\n",
    "# func = lambda x:((edges['X2'] == x[0]) & (edges['X1'] == x[1])).sum()\n",
    "\n",
    "# df_train['reversed'] = df_train[['X1', 'X2']].swifter.apply(func, axis=1)\n",
    "# df_test['reversed'] = df_test[['X1', 'X2']].swifter.apply(func, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Adamic/Adar Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pairwise similarities - Done\n",
    "# M = G.similarity_inverse_log_weighted(mode='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['adar'] = df_train[['X1', 'X2']].apply(similarity, axis=1, M=M)\n",
    "# df_test['adar'] = df_test[['X1', 'X2']].apply(similarity, axis=1, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pairwise similarities with weak connections - done\n",
    "M = G.similarity_inverse_log_weighted(mode='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['adar_weak'] = df_train[['X1', 'X2']].apply(similarity, axis=1, M=M)\n",
    "df_test['adar_weak'] = df_test[['X1', 'X2']].apply(similarity, axis=1, M=M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pairwise similarities - Done\n",
    "# M = G.similarity_jaccard(mode='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['jaccard'] = df_train[['X1', 'X2']].apply(similarity, axis=1, M=M)\n",
    "# df_test['jaccard'] = df_test[['X1', 'X2']].apply(similarity, axis=1, M=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pairwise similarities with weak connections - Done\n",
    "# M = G.similarity_jaccard(mode='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['jaccard_weak'] = df_train[['X1', 'X2']].apply(similarity, axis=1, M=M)\n",
    "# df_test['jaccard_weak'] = df_test[['X1', 'X2']].apply(similarity, axis=1, M=M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Save to the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('df_train_temp.csv', index=None)\n",
    "df_test.to_csv('df_test_temp.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Combine all the features above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Read saved features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv('df_info_temp.csv')\n",
    "df_train = pd.read_csv('df_train_temp.csv')\n",
    "df_test = pd.read_csv('df_test_temp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Assign node features to the dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless features\n",
    "df_features = df_info.drop(['Title', 'Author', 'Journal', 'Abstract', 'index'], axis=1)\n",
    "\n",
    "df_train = df_train.merge(df_features, left_on='X1', right_on='ID', how='left')\n",
    "df_train = df_train.merge(df_features, left_on='X2', right_on='ID', how='left', suffixes=('_X1', '_X2'))\n",
    "df_test = df_test.merge(df_features, left_on='X1', right_on='ID', how='left')\n",
    "df_test = df_test.merge(df_features, left_on='X2', right_on='ID', how='left', suffixes=('_X1', '_X2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Drop Doc2vecs columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_del = [c for c in df_train.columns if c[:2]=='v_']\n",
    "df_train = df_train.drop(cols_del, axis=1)\n",
    "df_test = df_test.drop(cols_del, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Drop redundant indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['X1', 'X2'], axis=1)\n",
    "df_test = df_test.drop(['X1', 'X2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Drop useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['shortest_path'], axis=1)\n",
    "df_test = df_test.drop(['shortest_path'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop('Y', axis=1)\n",
    "X_test = df_test\n",
    "y = df_train[['Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data (615512, 30)\n",
      "Shape of the test data (32648, 30)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the training data', X.shape)\n",
    "print('Shape of the test data', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['similarity_A_B', 'similarity_Aout_B', 'similarity_A_Bin', 'aut_common',\n",
       "       'n_previously_cited', 'reversed', 'adar', 'jaccard', 'jaccard_weak',\n",
       "       'adar_weak', 'ID_X1', 'Year_X1', 'betweenness_centrality_X1',\n",
       "       'eigenvector_centrality_X1', 'in_closeness_centrality_X1',\n",
       "       'in_degree_centrality_X1', 'out_closeness_centrality_X1',\n",
       "       'out_degree_centrality_X1', 'pagerank_X1', 'infomap_X1', 'ID_X2',\n",
       "       'Year_X2', 'betweenness_centrality_X2', 'eigenvector_centrality_X2',\n",
       "       'in_closeness_centrality_X2', 'in_degree_centrality_X2',\n",
       "       'out_closeness_centrality_X2', 'out_degree_centrality_X2',\n",
       "       'pagerank_X2', 'infomap_X2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Save to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('X.csv', index=None)\n",
    "X_test.to_csv('X_test.csv', index=None)\n",
    "y.to_csv('y.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
